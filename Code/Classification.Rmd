---
title: "Classification"
author: "Emilie Manzo"
date: "2024-09-18"
output: html_document
---

```{r load packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(momentuHMM)
library(bayesmove)
library(terra)
library(ggspatial)
library(ggmap)
library(ggrepel)
library(ggforce)
library(gridExtra)
library(ggpubr)
library(grid)
library(factoextra)
library(geosphere)
library(MASS)
library(caret)
library(sf)
library(magick)
library(zCompositions)
library(multcompView)
library(compositions)
library(git2r)
library(fs)
```

# 1- Extract feeding events from location data
## A) Data preparation
First, we will prepare the data streams for the model. To do so, we will import accelerometer data (containing location information) and compile it in one file per device number, keep only the location when the bird is out of the colony, split in distinct data sets according to resolution (i.e. the time difference between each data point, which were either 1 minute and less or 5 minutes), and finally remove outliers.

```{r Prep data, message=FALSE, warning=FALSE}
# If stored several files for the same individual in the same folder
import_accelerometer_data <- function(folder_name){
  setwd(paste0(path,folder_name))
  data <- data.frame()
  names <- list.files()

  for (i in 1:length(names)){
    data_fraction <- read.csv(names[i])
    data_fraction <- data_fraction[,-c(11:16,19:21)] #removes unnecessary columns, keeping location information (latitude, longitude, ...)
    data <- data %>% bind_rows(data_fraction)
  }
  data$device_info_serial <- as.factor(data$device_info_serial)
  data$date_time <- as.POSIXct(data$date_time)
  data$date <- as.Date(data$date_time, tz = "CET")
  data <- data %>% arrange(device_info_serial, date_time)
  return(data)
}
# accelerometer_data_6387 <- import_accelerometer_data("6387")

# Here, only trial file to upload:
Trial <- read.csv('https://raw.githubusercontent.com/EManzo13/Caspian-gull-NL/refs/heads/main/Trial_datasets/Trial_accelerometer.csv')

# Prepare dataset for the model, saved automatically as "gullsubs_1min" and "gullsubs_5min"
datasets_HMM <- function(data){
  # Format date and time
  data$counttime <- as.numeric(as.POSIXct(data$date_time)) # unit is seconds

  # Convert longitude latitude to UTM
  data <- data %>%
    mutate(v = list(vect(., c("longitude", "latitude"), crs = "+proj=longlat")),
           u = list(project(v[[1]], "+proj=utm +zone=31")),
           x = crds(u[[1]])[, 1],
           y = crds(u[[1]])[, 2]) %>%
    dplyr::select(-v, -u) %>%
    dplyr::select(1:4, x, y, everything())
  
  # Label colony observations within UTM coordinates
  data <- data %>%
    mutate(colony = ifelse(x >= 647500 & x <= 653000 & y >= 5850000 & y <= 5854500, "Yes", "No")) %>%
    dplyr::select(1:6, colony, everything())
  data$colony = as.factor(data$colony)
  
  # Sample smaller data set: every 60 seconds minimum and out of the colony
  interval <- 60 # in seconds
  data <- data %>%
    mutate(time_diff = counttime - lag(counttime, default = first(counttime))) %>%
    mutate(cumu_sum = cumsum(time_diff))
  gullout <- data %>%
    filter(cumu_sum %/% interval != lag(cumu_sum %/% interval, default = -1)) %>%
    filter(colony == "No") %>% dplyr::select(-time_diff, -cumu_sum,-colony)
  gullout <- gullout %>%
    mutate(time_diff = counttime - lag(counttime, default = first(counttime))) #recalculate new time difference with selected observations
  gullout <- gullout %>%
    mutate(dist_diff = c(0, distGeo(matrix(c(longitude[-n()], latitude[-n()]), ncol = 2), 
                                    matrix(c(longitude[-1], latitude[-1]), ncol = 2)))) #calculate distance difference
  
  # Sequence events according to resolution, time difference and dist difference and digitise them
  gullout$ID = 1
  for (i in 3:nrow(gullout)) {
    if (gullout$time_diff[i] < gullout$time_diff[i-1] & gullout$time_diff[i] > (gullout$time_diff[i-1]-120) 
        | gullout$time_diff[i] > gullout$time_diff[i-1] & gullout$time_diff[i] < (gullout$time_diff[i-1]+120)) {
      gullout$ID[i] <- gullout$ID[i-1]}
    else if (gullout$time_diff[i-1] > 2000 | gullout$time_diff[i-1] < 0){gullout$ID[i] <- gullout$ID[i-1]}
    else if (gullout$time_diff[i] > 400 & gullout$time_diff[i] > (gullout$time_diff[i-1]+120) 
             & gullout$dist_diff[i] < 5000 & gullout$time_diff[i] < 2000) {
      gullout$ID[i] <- gullout$ID[i-1]}
    else if (gullout$time_diff[i-1] > 400 & gullout$time_diff[i-1] < 2000 & gullout$time_diff[i] < (gullout$time_diff[i-1]-120) 
             & gullout$dist_diff[i] < 5000) {
      gullout$ID[i] <- gullout$ID[i-1]}
    else if (gullout$time_diff[i] == gullout$time_diff[i-1]){gullout$ID[i] <- gullout$ID[i-1]}
    else {gullout$ID[i] <- gullout$ID[i-1] + 1}
  }
  gullout$ID <- as.factor(gullout$ID)

  
  # Dividing in 2 data sets according to resolution
  ## Group data by sequence ID and calculate mean time_diff for each sequence, excluding the first row (as it's 0) 
  ID_mean_timediff <- gullout %>%
    group_by(ID) %>%
    filter(row_number() != 1) %>% # Exclude the first row of each sequence
    summarise(mean_time_diff = mean(time_diff, na.rm = TRUE)) %>%
    ungroup()
  gullout <- gullout %>%
    left_join(ID_mean_timediff, by = "ID")
  unique(gullout$ID_mean_timediff) #see the most common resolutions, here two different: around 1 min and around 5 min -> 2 different data sets (high and low resolution)
  gullout <- gullout %>% mutate(resolution = ifelse(mean_time_diff < 80, "High", "Low")) %>%
    relocate(1:4,mean_time_diff,resolution,everything())
  gullout <- gullout %>% mutate(speed = dist_diff/time_diff)
  
  ## Filter sequences into two datasets based on mean time differences found
  threshold_1min <- 60
  threshold_5min <- 280
  gullsubs_1min <- gullout %>%
    filter(abs(mean_time_diff - threshold_1min) <= abs(mean_time_diff - threshold_5min)) %>%
    dplyr::select(-mean_time_diff)
  gullsubs_5min <- gullout %>%
    filter(abs(mean_time_diff - threshold_5min) < abs(mean_time_diff - threshold_1min)) %>%
    dplyr::select(-mean_time_diff)
  
  # Remove sequences with less than 3 observations
  gullsubs_1min <<- gullsubs_1min %>%
    group_by(ID) %>%
    filter(n() >= 3) %>%
    ungroup()
  gullsubs_5min <<- gullsubs_5min %>%
    group_by(ID) %>%
    filter(n() >= 3) %>%
    ungroup()
}

# Remove outliers based on unrealistic speed (≥ 28 m/s) or altitude (≥ 25 000 m)
rm_outliers <- function(data){
  data <- data %>% mutate(row = row_number())
  data$date <- as.Date(data$date_time, tz = "CET")
  
  outliers <- data %>%
    filter(speed > 28 | altitude > 25000) %>%
    group_by(date) %>%
    mutate(outlier_id = row_number())
  outliers <- outliers %>%
    group_by(date) %>%
    mutate(mark_for_removal = if_else(outlier_id < n(), TRUE, FALSE)) %>%
    ungroup() %>% filter(mark_for_removal == TRUE)
  outliers_index <- outliers$row
  if(length(outliers_index) >0){
    data <- data[-which(data$row %in% outliers_index),]
  }
  return(data)
}

datasets_HMM(Trial)
gullsubs_1min <- rm_outliers(gullsubs_1min)
gullsubs_5min <- rm_outliers(gullsubs_5min)
```

## B) Apply hidden Markov model (HMM)
Based on the method developed by McClintock & Michelot (2018), we used a hidden Markov model (HMM) to detect area-restricted search (ARS) moments from stationary moments and transit & exploratory flights. We used the mumentuHMM package (McClintock & Michelot, 2022) developed by the authors for this purpose.
It uses the step length and turning angle parameters, therefore using latitude and longitude collected by the device.
Theses parameters are fixed for each of the 4 states we wish to differentiate (Area-restricted search, Stationary, Exploratory flight and Transit flight) and selected to be the best fitting parameters, determined with randomly generated parameters and maximum likelihood on a subset of the data (see "Models building" code file, which details this process).
Then, the Viterbi algorithm is used to extract the state of each data point determined by the model.

```{r HMM, message=FALSE, warning=FALSE}
# Import models with saved parameters from GitHub
best_model1 <- readRDS(gzcon(url("https://github.com/EManzo13/Caspian-gull-NL/raw/main/Models/best_model_1min.rds")))
best_model5 <- readRDS(gzcon(url("https://github.com/EManzo13/Caspian-gull-NL/raw/main/Models/best_model_5min.rds")))
# To import models from working directory
# best_model1 <- readRDS("best_model_1min.rds")
# best_model5 <- readRDS("best_model_5min.rds")

fit_hmm <- function(data,best_model, same_device = TRUE){ #same_device argument is used when all data doesn't have the same device number but are still grouped in the same dataset
  if(same_device == F){ 
    data <- data %>%
      mutate(device_real = device_info_serial,
             device_info_serial = device_info_serial[1])} #must be the same device nb for all data, or following df_to_list creates 1 dataframe per animal, which isn't supported by the model
  
  d.list<- df_to_list(data, ind = "device_info_serial") 
  list1<- map(d.list, ~prepData(., type = "UTM", coordNames = c("x","y")))
  
  stepPar <- as.vector(best_model$mle$step[c(1,3,5,7,2,4,6,8)]) #saves the parameters defined in the best model in the right order for application
  if (length(which(list1[[1]]$step == 0)) > 0){
    zero_mass <- rep(length(which(list1[[1]]$step == 0))/length(list1[[1]]$step),4)
    stepPar <- c(stepPar,zero_mass)
  }
  anglePar <- as.vector(abs(best_model$mle$angle[c(1,3,5,7,2,4,6,8)]))
  dist <- best_model$conditions$dist
  
  HMM_model <- fitHMM(data = list1[[1]],
                       nbStates = 4, 
                       Par0 = list(step = stepPar, angle = anglePar),
                       fixPar = list(step = stepPar, angle = anglePar),
                       dist = dist,
                       formula = ~ 1,
                       stationary=TRUE,
                       estAngleMean = list(angle=TRUE),
                       stateNames = c("ARS", "Stationary", "Exploratory", "Transit"),
                       optMethod = "Nelder-Mead") 
  
  if(same_device == F){
    data$device_info_serial <- data$device_real
    data <- data %>% dplyr::select(-device_real)
  }
  
  HMM_state <- viterbi(HMM_model)
  state_mapping <- c("1" = "ARS", "2" = "Stationary", "3" = "Exploratory", "4" = "Transit")
  HMM_state <- state_mapping[HMM_state]
  return(HMM_state)
}
gullsubs_1min$HMM_state <- fit_hmm(gullsubs_1min,best_model1) #adds HMM_states column to the dataset
gullsubs_5min$HMM_state <- fit_hmm(gullsubs_5min,best_model5)

# Re-assemble the two datasets in one, and link same tracks with different resolutions back together 
group_tracks <- function(dataset_1min = gullsubs_1min, dataset_5min = gullsubs_5min){
  # Merge datasets with states detected by model
  HMM_states <- gullsubs_1min %>% dplyr::select(device_info_serial, date_time, time_diff, 
                                                HMM_state, ID, resolution, dist_diff, x,y,latitude,longitude,
                                                temperature, altitude,date,counttime) %>%
    bind_rows(gullsubs_5min %>% dplyr::select(device_info_serial, date_time, time_diff,
                                              HMM_state, ID, resolution,dist_diff,x,y,latitude,longitude,
                                              temperature, altitude,date,counttime)) %>%
    arrange(ID,date_time)
  HMM_states$HMM_state <- as.factor(HMM_states$HMM_state)
  HMM_states <- HMM_states %>%
    mutate(time_diff = counttime - lag(counttime, default = first(counttime))) %>% # Recalculate new time difference with selected observations
    mutate(dist_diff = c(0, distGeo(matrix(c(longitude[-n()], latitude[-n()]), ncol = 2), 
                                    matrix(c(longitude[-1], latitude[-1]), ncol = 2))))
  
  # Identify complete tracks (events seperated by less than 15 min)
  HMM_states$track[1] = 1
  for (i in 2:nrow(HMM_states)) {
    if (HMM_states$time_diff[i] < 1500 & HMM_states$time_diff[i] >= 0) {
      HMM_states$track[i] <- HMM_states$track[i-1]}
    else {HMM_states$track[i] <- HMM_states$track[i-1] + 1}
  }
  
  HMM_states <- HMM_states %>%
    relocate(1:2, track, HMM_state, ID, time_diff, everything())
  return(HMM_states)
}

HMM_states <- group_tracks()
```

The reassembled tracks can be plotted to see the efficiency of the model.

```{r Plot tracks, warning=FALSE}
stateNames <- c("ARS", "Stationary", "Exploratory", "Transit")
myColors <- c("ARS" = "#FB8072","Stationary" = "#B3DE69", "Exploratory" = "#FDB462","Transit" = "#80B1D3")
track_plot <- function(data,track){
  cur_set <- which(data$track == track)
  subset <- data[cur_set,]
  plot <- ggplot(subset, aes(x = longitude, y = latitude)) +
    geom_segment(aes(xend = lead(longitude), yend = lead(latitude), col = HMM_state)) +
    geom_point(aes(col = HMM_state), size = 1.5) +
    scale_color_manual(name = "Activity", values = myColors) +
    labs(
      title = paste0("Route of track ", track),
      x = "Longitude",
      y = "Latitude",
      subtitle = paste("Device ", unique(subset$device_info_serial), ", from ", min(subset$date_time), " until ", max(subset$date_time), sep = "")) +
    theme_classic()
  return(plot)
}

# All tracks
for(i in min(HMM_states$track):max(HMM_states$track)){
  if(i <= max(HMM_states$track)){
    plot <- track_plot(HMM_states,i)
  print(plot)
  }
  else {stop()}
}

## Plot a specific track for a certain device and date
# HMM_states %>% filter(device_info_serial == 6387 & date == "2023-05-01") %>%
#   pull(track) %>%
#   unique() -> track_nb #get track number with device nb and date
# track_plot(HMM_states,track_nb)

```

## C) Final output of the first model
Now that each data point has been matched with an activity type by the HMM, we want to extract the start and end time of each feeding event in a .txt document with 3 columns: device number, start date and time, and end date and time. This document is automatically saved in the working directory under the name "input_devicenb.csv"; and the function returns a table with a summary of all detected events with these three columns as well as location information.
The output file should be the same as the "HMM_output.csv" file in the "Trial_datasets" folder.

```{r Feeding events sequence, message=FALSE, warning=FALSE}
feeding_sequences <- function(data = HMM_states,last_id_data = 0){ #last_id_data argument for digitisation of tracks following the ones previously done

  # Keep the ARS sequences and data points around it, remove full Transit sequences
  data$keep <- FALSE
  n <- nrow(data)
  device <- data$device_info_serial[1]
  for (i in 1:n) {
    if (data$HMM_state[i] %in% c("ARS", "Stationary", "Exploratory")) {
      data$keep[i] <- TRUE
    } else if (data$HMM_state[i] == "Exploratory " && i > 1 && i < n && data$HMM_state[i - 1] == "Transit" && data$HMM_state[i + 1] == "Transit"){
      data$keep <- FALSE
    } else if (data$HMM_state[i] == "Transit") {
      if (i > 1 && data$HMM_state[i - 1] %in% c("ARS", "Stationary", "Exploratory")) {
        data$keep[i] <- TRUE
      } else if (i < n && data$HMM_state[i + 1] %in% c("ARS", "Stationary", "Exploratory")) {
        data$keep[i] <- TRUE
      } else if (i > 2 && data$resolution[i] == "High" && data$HMM_state[i - 2] %in% c("ARS", "Stationary", "Exploratory")) {
        data$keep[i] <- TRUE
      } else if (i < n - 1 && data$resolution[i] == "High" && data$HMM_state[i + 2] %in% c("ARS", "Stationary", "Exploratory")) {
        data$keep[i] <- TRUE
      }
    }
  }
  active <- data[data$keep == TRUE, ]
  
  # Merge consecutive ARS sequences into the same feeding event (based on time and location)
  active <- active %>%
    mutate(time_diff = counttime - lag(counttime, default = first(counttime))) %>%   # Recalculate new time difference with selected observations
    mutate(dist_diff = c(0, distGeo(matrix(c(longitude[-n()], latitude[-n()]), ncol = 2), 
                                    matrix(c(longitude[-1], latitude[-1]), ncol = 2))))
  active$event_seq = 1
  for (i in 3:nrow(active)) {
    if (active$longitude[i] > 5.41953867 && active$latitude[i] > 52.87209579) {
      if (active$dist_diff[i] > 4000 | active$time_diff[i] > 1800){
        active$event_seq[i] <- active$event_seq[i-1] + 1}
      else {active$event_seq[i] <- active$event_seq[i-1]}
    }
    else {
      if (active$track[i] != active$track[i-1]){
        active$event_seq[i] <- active$event_seq[i-1] + 1}
      else if (active$resolution[i] == "High" & active$time_diff[i] >= 0 & active$time_diff[i] <= 90) {
        active$event_seq[i] <- active$event_seq[i-1]}
      else if (active$resolution[i] == "High" & active$time_diff[i] > 90 & active$time_diff[i] <= 400 & active$dist_diff[i] <= 300) {
        active$event_seq[i] <- active$event_seq[i-1]}
      else if (active$resolution[i] == "Low" & active$time_diff[i] >= 0 & active$time_diff[i] <= 400 & active$dist_diff[i] < 500){
        active$event_seq[i] <- active$event_seq[i-1]}
      else if (active$resolution[i] == "Low" & active$time_diff[i] > 400 & active$time_diff[i]<= 1000 & active$dist_diff[i] < 1000) {
        active$event_seq[i] <- active$event_seq[i-1]}
      else if (active$dist_diff[i] > 3000){
        active$event_seq[i] <- active$event_seq[i-1] + 1}
      else if (active$time_diff[i] > 1000){
        active$event_seq[i] <- active$event_seq[i-1] + 1}
      else {active$event_seq[i] <- active$event_seq[i-1] + 1}
    }
  }
  
  # Check events with more than 1 ARS sequence are within a certain distance
  active <- active %>% mutate(row = row_number()) 
  terrestrial <- active %>%
    filter(HMM_state %in% c("ARS", "Stationary"))
  terrestrial <- terrestrial %>%
    mutate(dist_diff = c(0, distGeo(matrix(c(longitude[-n()], latitude[-n()]), ncol = 2), 
                                    matrix(c(longitude[-1], latitude[-1]), ncol = 2))))
  unique_event_seqs <- unique(terrestrial$event_seq)
  new_event_seq <- c()
  last_id <- active$event_seq[nrow(active)]
  for (seq_id in unique_event_seqs) {
    event_data <- terrestrial %>% filter(event_seq == seq_id)
    if (nrow(event_data) > 1){
      if (max(event_data$longitude) < 5.41953867 && max(event_data$latitude) < 52.87209579) {
        for (j in 2:nrow(event_data)){
          if (event_data$dist_diff[j] > 500){
            event_data$event_seq[j:nrow(event_data)] <- last_id + 1
            last_id <- last_id + 1}
          else {event_data$event_seq[j] <- event_data$event_seq[j]}
        }
      }
    }
    new_event_seq <- c(new_event_seq,event_data$event_seq)
  }
  
  # Digitise the selected sequences
  terrestrial$new_event_seq <- new_event_seq
  max_ori_event_seq <- max(active$event_seq)
  active <- active %>%
    left_join(terrestrial %>% dplyr::select(row, new_event_seq), by = "row")
  active <- active %>%
    arrange(row) %>%  # Ensure the rows are in the correct order
    mutate(event_seq = coalesce(new_event_seq, event_seq)) 
  analysed_event_seqs <- c(1)
  for (i in 2:nrow(active)) {
    if (active$event_seq[i] != active$event_seq[i - 1]) {
      # Update analysed_event_seqs only if the event_seq changes
      analysed_event_seqs <- c(analysed_event_seqs, active$event_seq[i])
    }
    # Check if the current event_seq should be updated based on the previous event_seq
    if (active$event_seq[i - 1] > max_ori_event_seq &&
        active$event_seq[i] %in% analysed_event_seqs[1:length(analysed_event_seqs)-1]) {
      active$event_seq[i] <- active$event_seq[i - 1]
    }
  }
  
  # Only keep events with at least 1 ARS in
  filtered_active <- active %>%
    group_by(event_seq) %>%
    filter(any(HMM_state == "ARS")) %>%
    ungroup()
  filtered_active <- filtered_active %>%
    arrange(event_seq) %>%
    mutate(event_id = as.numeric(factor(event_seq)))
  filtered_active <- filtered_active %>%
    relocate(1:5, event_id, everything())
  
  # Get start and end date & time of each event, as well as the event's median location
  feeding_events <- filtered_active %>%
    group_by(device_info_serial, event_id) %>%
    summarise(
      start_time = min(date_time),
      end_time = max(date_time),
      x = median(x),
      y = median(y),
      latitude = median(latitude),
      longitude = median(longitude),
      .groups = 'drop')
  feeding_events <- feeding_events %>% arrange(start_time) %>% mutate(event_id = row_number())
  feeding_events <- feeding_events[which(feeding_events$start_time != feeding_events$end_time),] #removes event w/ 1 point
  
  # Save the final table as a .csv table in the working directory
  feeding_events$device_info_serial <- as.numeric(as.character(feeding_events$device_info_serial))
  output_file_name <- paste0("input_",device,".csv")
  feeding_events %>% dplyr::select(device_info_serial,start_time,end_time) %>%
    write.table(output_file_name, sep = ",", col.names = FALSE, row.names = FALSE)
  feeding_events$device_info_serial <- as.factor(feeding_events$device_info_serial)
  return(feeding_events)
}
events_seq_trial <- feeding_sequences()
print(events_seq_trial)
```

# 2- Behavioural model

The last file produced and automatically saved is used as in input in the behavioural model to obtain the behavioural sequence of each feeding event. The output is one file per event, with each line being a location point and the associated behaviour. All files for one individual are stored in the same folder. 

# 3- Classification of the events
## A) Import and compile
First, we will import one by one and merge the files put out by the behavioural model to have one file per individual (one file for the trial).
It is noteworthy that as outputs of the behavioural model, the files are named with the device number, starting and ending date & time (for example: '6387_2023-05-01 07/32/10_2023-05-01 07/46/34.csv'). The file name is used here to fill in the columns "device number", "start date & time" and "end date & time". It is then used to match with the events detected by the HMM to obtain the ID of the event that was previously attributed (in the table obtained with the "feeding_sequences()" function).

```{r Merge files,warning=FALSE, message=FALSE,echo=T,results="hide",quiet=TRUE}

import_and_merge <- function(feeding_sequences_table){
  # Set the GitHub repository and folder path
  folder_path = "Trial_datasets/Behavioural_model_output"
  repo_url <- "https://github.com/EManzo13/Caspian-gull-NL"

  # Clone the GitHub repository to a temporary directory
  temp_import_dir <- tempfile("githubRepo")
  clone(url = repo_url, local_path = temp_import_dir)

  # Construct the path and get the list of files in the folder
  data_dir <- file.path(temp_import_dir, folder_path)
  files_list <- dir_ls(path = data_dir, glob = "*.csv")

  # Import and merge
  all_behav_events <- data.frame()
  imported_event_ids <- numeric()
  
  for (file_path in files_list){
    behav_event <- read.csv(file_path)
    behav_event$date_time <- as.POSIXct(behav_event$date_time)
    
    file_info <- strsplit(basename(file_path), "_\"|\"_\"|\".csv")[[1]]
    device <- file_info[1]
    start_time_file <- as.POSIXct(file_info[2], format = "%Y-%m-%d %H:%M:%OS")
    end_time_file <- as.POSIXct(file_info[3], format = "%Y-%m-%d %H:%M:%OS")
    
    matched_event <- feeding_sequences_table %>%
      filter(device_info_serial == device,
             start_time <= start_time_file,
             end_time >= end_time_file)
    
    if (nrow(matched_event) == 1) {
      event_id <- matched_event$event_id
      
      # Check if event_id already exists in all_behav_events
      if (!(event_id %in% imported_event_ids)) {
        behav_event$event_id <- event_id
        behav_event <- behav_event %>% relocate(event_id, everything())
        
        all_behav_events <- bind_rows(all_behav_events, behav_event)
        imported_event_ids <- c(imported_event_ids, event_id)
      } else {
        message(paste("Event_id", event_id, "already imported. Skipping duplicate."))
      }
    } else if (nrow(matched_event) == 0) {
      message(paste("Event_id", event_id, "filtered out. Skipping"))
    } else {
      cat(nrow(matched_event)," event(s) found,\nLast working track:", max(all_behav_events$event_id),"\nLast imported file:",i)
      stop()}
  }
  return(all_behav_events)
}
Trial_behaviour_combined <- import_and_merge(events_seq_trial)
```

```{r print importation result,warning=FALSE, message=FALSE,echo=T}
# Check all events detected by the HMM are imported: number of events per individual in both tables
Trial_behaviour_combined %>%
  group_by(device_info_serial) %>%
  summarise(n_events_seq = n_distinct(event_id)) %>% 
  bind_cols(events_seq_trial %>% group_by(device_info_serial) %>% 
              summarise(n_events_imported = n()))
```

## B) Filter and compute proportions 
We will now filter the behavioural observations to mark the ones with a low interpretation confidence (estimated by the model) as Unknown, as well as the improbable ones (i.e. the ones with a high altitude and sitting behaviour).
Then, we will reformat the behavioural data per folder to have one file per individual, with one line per event containing the proportions of each behaviour as well as the event ID. 

```{r Filter and compute proportions, message=FALSE, warning=FALSE}
filter_behav <- function(data){
  # Filter the data where index == 0 and remove interpretation from the other model
  data_filtered <- data %>% filter(index == 0)
  
  # Mutate the prediction column based on confidence and altitude, and remove the impossible ones
   data_filtered <- data_filtered %>% 
    mutate(prediction = ifelse(confidence < 0.60, "Unknown", prediction)) %>% 
    mutate(altitude = ifelse(altitude > 50 & prediction == "SitStand" & confidence >= 0.80, 0, altitude)) %>% 
    mutate(prediction = ifelse(altitude > 50 & prediction == "SitStand" & confidence < 0.80, "Unknown", prediction)) %>% 
    mutate(prediction = ifelse(prediction == "Boat", "Unknown", prediction))
 
  # Format the variables
  data_filtered$device = as.factor(data_filtered$device)
  data_filtered$prediction = as.factor(data_filtered$prediction)
  
  return(data_filtered)
}
Trial_filtered <- filter_behav(Trial_behaviour_combined)

behaviour_proportions <- function(data, feeding_sequences_table) {
  data <- data %>% dplyr::filter(prediction != "Unknown")
  behaviour_counts <- data %>%
    group_by(event_id, prediction) %>%
    summarise(count = n(), .groups = 'drop')

  total_counts <- behaviour_counts %>%
    group_by(event_id) %>%
    summarise(total = sum(count), .groups = 'drop')

  behaviour_proportions <- behaviour_counts %>%
    left_join(total_counts, by = "event_id") %>%
    mutate(proportion = count / total) %>%
    dplyr::select(event_id, prediction, proportion)

  # Reshape the data to wide format
  reshaped_proportions <- behaviour_proportions %>%
    pivot_wider(names_from = prediction, values_from = proportion, values_fill = 0) %>%
    mutate(across(-event_id, ~ . * 100)) %>%
    arrange(event_id)
  reshaped_proportions <- reshaped_proportions %>%
    relocate(event_id,SitStand,TerLoco,Float,Pecking,Flap,Soar,ExFlap,Manouvre)
  
  # Add event info from earlier based on event id
  reshaped_proportions <- left_join(reshaped_proportions, feeding_sequences_table, by = "event_id")
  
  # Remove events without terrestrial behaviour
  if(length(which(reshaped_proportions$Flap + reshaped_proportions$ExFlap + reshaped_proportions$Soar + reshaped_proportions$Manouvre == 100))>0){
    reshaped_proportions <- reshaped_proportions[-which(reshaped_proportions$Flap + reshaped_proportions$ExFlap + reshaped_proportions$Soar + reshaped_proportions$Manouvre == 100),]
      reshaped_proportions <- reshaped_proportions %>% arrange(start_time) %>% mutate(event_id = row_number())
  }

  return(reshaped_proportions)
}
Trial_summary <- behaviour_proportions(Trial_filtered, events_seq_trial)
print(Trial_summary)
```

## C) Classification
As explained in the thesis, the classification is done partially based on location (for events in a waste management plant, in the Ijsselmeer or on its coast, and in smaller inland lakes) or based on behaviour proportions (for inland events, in agricultural fields or canals).
The lakes polygons are obtained from the HydroLakes database, the waste management plants polygons are created in the following code, and the LDA model based on behaviour to differenciate agricultural fields and canals is trained on a trial dataset annotated by hand (see "Models building" code file).
The input of the classifying function should be a table containing information on location as well as behaviour proportions.

```{r classification, message=FALSE, warning=FALSE, ,results="hide",quiet=TRUE}
  
load_classification_objects <- function(){ #Load required datasets, model and function
  
  # Set the GitHub repository and folder path
  folder_path = "Models"
  repo_url <- "https://github.com/EManzo13/Caspian-gull-NL"

  # Clone the GitHub repository to a temporary directory
  temp_import_dir <- tempfile("githubRepo")
  clone(url = repo_url, local_path = temp_import_dir)

  # Construct the path and get the list of files in the folder
  data_dir <- file.path(temp_import_dir, folder_path)
  files_list <- dir_ls(path = data_dir)
  
  # Import and check validity
  LDA_model <<- readRDS(files_list[1])
  
  lakes_buffer <<- readRDS(files_list[6])
  lakes_buffer <<- st_make_valid(lakes_buffer)
  
  ijsselmeer_coast <<- readRDS(files_list[4])
  st_crs(ijsselmeer_coast) <- st_crs(lakes_buffer)
  ijsselmeer_coast <<- st_make_valid(ijsselmeer_coast)
  
  ijsselmeer <<- readRDS(files_list[5])
  st_crs(ijsselmeer) <- st_crs(lakes_buffer)
  ijsselmeer <<- st_make_valid(ijsselmeer)
  
  # Define areas that need different buffer size (Agriculture fields close to the coast)
  coastal_agri_areas <<- st_sfc(
    st_polygon(list(rbind(c(5.09133928, 52.77384980), c(5.11180990, 52.77457775), c(5.12191121, 52.84782449), c(5.06827085, 52.90514619), c(5.05781206, 52.90153375), c(5.09787862, 52.84613331), c(5.09133928, 52.77384980)))),
    st_polygon(list(rbind(c(5.17694879, 52.74266369), c(5.18896509, 52.75768205), c(5.22312570, 52.75669917), c(5.23209434, 52.75632622), c(5.22883277, 52.74832997), c(5.19748697, 52.74971954), c(5.18452654, 52.74015506), c(5.17694879, 52.74266369)))),
    st_polygon(list(rbind(c(5.23825157, 52.68349655), c(5.25244771, 52.68324193), c(5.24249135, 52.65369382), c(5.16201076, 52.61870670), c(5.12767849, 52.61497096), c(5.10192928, 52.62664286), c(5.10776577, 52.63916130), c(5.14862117, 52.63438551), c(5.22552547, 52.66023106), c(5.23825157, 52.68349655)))),
    st_polygon(list(rbind(c(5.01369242, 52.61160676), c(5.02605204, 52.61171914), c(5.02944771, 52.59240372), c(5.07545296, 52.52186933), c(5.05931679, 52.51812533), c(5.01475888, 52.59257740), c(5.01369242, 52.61160676)))),
    st_polygon(list(rbind(c(5.49891787, 52.55687130), c(5.50591307, 52.55402928), c(5.56485286, 52.58746116), c(5.66128225, 52.61240403), c(5.63449307, 52.64914882), c(5.60977383, 52.64741085), c(5.62899991, 52.61573776), c(5.55691642, 52.59457487), c(5.49891787, 52.55687130)))),
    st_polygon(list(rbind(c(5.59022867, 52.67668654), c(5.59812509, 52.67700284), c(5.60567040, 52.76554717), c(5.66764015, 52.82837470), c(5.69836754, 52.83120742), c(5.69685685, 52.83611943), c(5.66235291, 52.83166010), c(5.58786474, 52.76480491), c(5.59022867, 52.67668654)))),
    st_polygon(list(rbind(c(5.65385167, 52.85397276), c(5.65771808, 52.85563124), c(5.64355199, 52.86257547), c(5.61608617, 52.85304793), c(5.58064410, 52.84939558), c(5.56558242, 52.83579410), c(5.54017654, 52.83652810), c(5.45551341, 52.85615149), c(5.41551182, 52.85117771), c(5.37036488, 52.87398593), c(5.36624500, 52.87254346), c(5.41534016, 52.84869780), c(5.45534175, 52.85242794), c(5.54154983, 52.82855064), c(5.56850067, 52.83331331), c(5.58545062, 52.84764128), c(5.61763112, 52.84879767), c(5.64320867, 52.85905977), c(5.65385167, 52.85397276))))
  )
  st_crs(coastal_agri_areas) <- st_crs(lakes_buffer)
  coastal_agri_areas <<- st_make_valid(coastal_agri_areas)
  
  # Define waste management plants locations
  garbage <<- st_sfc(
    st_polygon(list(rbind(c(5.06082061, 52.77239580), c(5.07343773, 52.77252764), c(5.07210735, 52.76863505), c(5.06760124, 52.76692131), c(5.06060604, 52.76850523), c(5.06082061, 52.77239580)))),
    st_polygon(list(rbind(c(5.24583702, 52.41666079), c(5.25343304, 52.41975134), c(5.25660877, 52.41776421), c(5.24832611, 52.41443998), c(5.24583702, 52.41666079)))),
    st_polygon(list(rbind(c(4.75742737, 52.60589692), c(4.76298490, 52.60467299), c(4.75856069, 52.59685102), c(4.75323919, 52.60099557), c(4.75742737, 52.60589692)))),
    st_polygon(list(rbind(c(5.06312263, 52.67185655), c(5.06726396, 52.67235800), c(5.06852997, 52.67002941), c(5.06444228, 52.66960703), c(5.06312263, 52.67185655)))),
    st_polygon(list(rbind(c(5.26978458, 52.69643292), c(5.27315343, 52.69604279), c(5.27313189, 52.69441723), c(5.26977377, 52.69448926), c(5.26978458, 52.69643292)))),
    st_polygon(list(rbind(c(4.78194988, 52.79300250), c(4.78574789, 52.79301573), c(4.78569956, 52.79134451), c(4.78185328, 52.79137046), c(4.78194988, 52.79300250)))),
    st_polygon(list(rbind(c(4.77784276, 52.70404775), c(4.78102923, 52.70476335), c(4.78274778, 52.70236381), c(4.77946475, 52.70149914), c(4.77784276, 52.70404775))))
  )
  st_crs(garbage) <<- st_crs(lakes_buffer)
  garbage <<- st_make_valid(garbage)
  
  # Create a logration transformation function, used on the dataset before applying the LDA model
  logratio_transfo <<- function(data, id_col_name){
    data <- data %>% relocate(all_of(id_col_name), everything())
    # Replace the zeros
    transformed_data <- data %>%
      mutate(across(-id_col_name, ~ ifelse(. == 0, 0.001, .))) # Replace zeros with 0.001
    dl <- rep(0.001, ncol(transformed_data) - 1)
    
    # Apply multiplicative lognormal replacement for zeros
    transformed_data[,2:ncol(transformed_data)] <- zCompositions::multLN(as.matrix(transformed_data[, 2:ncol(transformed_data)]), label = 0.001, dl = dl, z.warning = 1)
    
    #Apply centered log ratio transfo
    transformed_data[,2:ncol(transformed_data)] <- as.data.frame(compositions::clr(transformed_data[, 2:ncol(transformed_data)]))
    
    return(transformed_data)
  }
}

load_classification_objects()

classify_events <- function(data) {
  # Convert data to sf format
  data_sf <- sf::st_as_sf(data, coords = c("longitude", "latitude"), crs = 4326)
  data_sf <- st_transform(data_sf, st_crs(lakes_buffer))
  data_sf$longitude <- data$longitude
  data_sf$latitude <- data$latitude
  # Calculate distance to IJsselmeer coast
  distance_matrix <- st_distance(data_sf, ijsselmeer_coast)
  data_sf$distance_to_coastline <- apply(distance_matrix, 1, min)
  
  # Define location types with distance to coast according to agri areas
  data_sf$ijsselmeer <- lengths(st_within(data_sf, ijsselmeer)) > 0
  data_sf$other_lakes <- lengths(st_within(data_sf, lakes_buffer)) > 0
  data_sf$coastal_agri <- lengths(st_within(data_sf, coastal_agri_areas)) > 0
  data_sf$garbage <- lengths(st_within(data_sf, garbage)) > 0
  
  # Assign event type based on location type
  data_sf$location_type <- "Inland"
  data_sf$location_type[data_sf$ijsselmeer & data_sf$distance_to_coastline > 1000] <- "IJsselmeer"
  data_sf$location_type[data_sf$other_lakes] <- "Other Lake"
  data_sf$location_type[data_sf$distance_to_coastline <= 1000 & !data_sf$coastal_agri] <- "Coast"
  data_sf$location_type[data_sf$distance_to_coastline <= 400 & data_sf$coastal_agri] <- "Coast"
  data_sf$location_type[data_sf$garbage] <- "Garbage"
  
  # Predict event type for inland locations using LDA
  data_sf <- as.data.frame(data_sf)
  inland_data <- data_sf %>% filter(location_type == "Inland")
  LDA_dataset <- data_sf %>% dplyr::select(event_id,SitStand,TerLoco,Float,Pecking,Flap,Soar,ExFlap,Manouvre)
  LDA_dataset <- logratio_transfo(LDA_dataset, "event_id")
  LDA_dataset$location_type <- data_sf$location_type
  LDA_dataset <- LDA_dataset[which(LDA_dataset$location_type == "Inland"),] #sort out events NOT inland
  
  LDA_dataset <- LDA_dataset %>% dplyr::select(-event_id,-location_type) #keep only behaviour proportions
  inland_class <- predict(LDA_model, LDA_dataset)$class
  inland_data$inland_class <- inland_class
  inland_data <- inland_data %>% dplyr::select(event_id, inland_class)
  inland_data$inland_class <- as.character(inland_data$inland_class)
  
  data_sf <- left_join(data_sf, inland_data, by = "event_id")
  data_sf <- data_sf %>% mutate(event_type = ifelse(location_type == "Inland",inland_class,location_type))
  
  results_table <- data_sf %>% dplyr::select(event_id,event_type)
  return(results_table)
}

event_types <- classify_events(Trial_summary)
Trial_summary <- left_join(Trial_summary, event_types, by = "event_id") #adds event_types column
```

```{r print classification results,warning=FALSE, message=FALSE,echo=T}
# Final result table: event ID, individual, date, time, duration and event type
feeding_events <- Trial_summary %>% dplyr::select(event_id,device_info_serial,start_time,end_time,event_type)
feeding_events$duration.in.min <- (as.numeric(as.POSIXct(feeding_events$end_time, tz = "CET")) - as.numeric(as.POSIXct(feeding_events$start_time, tz = "CET")))/60
print(feeding_events)
```

# Bibliography
McClintock B and Michelot T (2022). ''momentuHMM: Maximum Likelihood Analysis of Animal Movement Behavior Using Multivariate Hidden Markov Models''. https://cran.r-project.org/web/packages/momentuHMM/index.html .    

McClintock B and Michelot T (2018). ''momentuHMM: R package for generalized hidden Markov models of animal movement'', Methods in Ecology and Evolution, 9(6), pp. 1518–1530. doi:10.1111/2041-210X.12995.
